#[cfg(feature = "v1")]
pub(crate) use llama_cpp_2_v1 as llama_cpp_2;

#[cfg(feature = "v2")]
pub(crate) use llama_cpp_2_v2 as llama_cpp_2;




mod sampler;
mod model;

mod context;
mod infer;

pub use model::{ModelInstance, ModelInstanceConfig };
pub use context::{LlamaContext, LlamaContextConfig};
pub use infer::InferBatch;

/// performing inferencing for Large Language Models
#[derive(Debug, Clone)]
pub struct InferencingParams {
    /// The maximum tokens that should be inferred.
    /// set the length of the prompt + output in tokens
    ///
    /// Note: the backing implementation may return less tokens.
    pub max_tokens: i32,

    /// The randomness with which the next token is selected.
    /// The temperature used to generate samples, use 0 for greedy sampling.
    pub temperature: Option<f32>,
    /// The number of possible next tokens the model will choose from.
    /// Only sample among the top K samples.
    pub top_k: Option<i32>,
    /// The probability total of next tokens the model will choose from.
    // Nucleus sampling probability cutoff
    pub top_p: Option<f32>,

    pub min_keep: Option<i32>,

    /// The amount the model should avoid repeating tokens.
    /// 1.0 = disabled
    pub penalty_repeat: Option<f32>,
    /// The number of tokens the model should apply the repeat penalty to.
    ///  last n tokens to penalize (0 = disable penalty, -1 = context size)
    pub penalty_last_n: Option<i32>,
    /// 0.0 = disabled
    pub penalty_freq: Option<f32>,
    /// 0.0 = disabled
    pub penalty_present: Option<f32>,

    /// RNG seed
    seed: Option<u32>,

}

impl Default for InferencingParams {
    fn default() -> InferencingParams {
        return InferencingParams {
            max_tokens: 256,
            temperature: None,
            top_k: None,
            top_p: None,
            min_keep: None,
            penalty_last_n: None,
            penalty_repeat: None,
            penalty_freq: None,
            penalty_present: None,
            seed: None,
        };
    }
}


#[derive(Default, Debug, Clone)]
pub struct InferencingResult {
    /// The text generated by the model
    pub text: Vec<u8>,
    /// Number of tokens in the prompt
    pub prompt_token_count: usize,
    /// Number of tokens generated by the inferencing operation
    pub generated_token_count: usize,

    pub secs: f32,

    pub speed: f32,
}

#[derive(Default, Debug, Clone)]
pub struct EmbeddingsResult {
    /// The embeddings generated by the request
    pub embeddings: Vec<Vec<f32>>,
    /// Number of tokens in the prompt
    pub prompt_token_count: usize,

    /// Number of tokens generated by the inferencing operation
    pub generated_token_count: usize,

    pub secs: f32,

    pub speed: f32,
}


